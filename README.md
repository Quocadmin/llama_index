# üóÇÔ∏è LlamaIndex ü¶ô

[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)
[![Build](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml/badge.svg)](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml)
[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)
[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)
[![Twitter](https://img.shields.io/twitter/follow/llama_index)](https://x.com/llama_index)
[![Reddit](https://img.shields.io/reddit/subreddit-subscribers/LlamaIndex?style=plastic&logo=reddit&label=r%2FLlamaIndex&labelColor=white)](https://www.reddit.com/r/LlamaIndex/)
[![Ask AI](https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,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)](https://www.phorm.ai/query?projectId=c5863b56-6703-4a5d-87b6-7e6031bf16b6)

LlamaIndex (GPT Index) is a data framework for your LLM application. Building with LlamaIndex typically involves working with LlamaIndex core and a chosen set of integrations (or plugins). There are two ways to start building with LlamaIndex in
Python:

1. **Starter**: [`llama-index`](https://pypi.org/project/llama-index/). A starter Python package that includes core LlamaIndex as well as a selection of integrations.

2. **Customized**: [`llama-index-core`](https://pypi.org/project/llama-index-core/). Install core LlamaIndex and add your chosen LlamaIndex integration packages on [LlamaHub](https://llamahub.ai/)
   that are required for your application. There are over 300 LlamaIndex integration
   packages that work seamlessly with core, allowing you to build with your preferred
   LLM, embedding, and vector store providers.

The LlamaIndex Python library is namespaced such that import statements which
include `core` imply that the core package is being used. In contrast, those
statements without `core` imply that an integration package is being used.

```python
# typical pattern
from llama_index.core.xxx import ClassABC  # core submodule xxx
from llama_index.xxx.yyy import (
    SubclassABC,
)  # integration yyy for submodule xxx

# concrete example
from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
```

### Important Links

LlamaIndex.TS [(Typescript/Javascript)](https://github.com/run-llama/LlamaIndexTS)

[Documentation](https://docs.llamaindex.ai/en/stable/)

[X (formerly Twitter)](https://x.com/llama_index)

[LinkedIn](https://www.linkedin.com/company/llamaindex/)

[Reddit](https://www.reddit.com/r/LlamaIndex/)

[Discord](https://discord.gg/dGcwcsnxhU)

### Ecosystem

- LlamaHub [(community library of data loaders)](https://llamahub.ai)
- LlamaLab [(cutting-edge AGI projects using LlamaIndex)](https://github.com/run-llama/llama-lab)

## üöÄ Overview

**NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!

### Context

- LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.
- How do we best augment LLMs with our own private data?

We need a comprehensive toolkit to help perform this data augmentation for LLMs.

### Proposed Solution

That's where **LlamaIndex** comes in. LlamaIndex is a "data framework" to help you build LLM apps. It provides the following tools:

- Offers **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.).
- Provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.
- Provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.
- Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, or anything else).

LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in
5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),
to fit their needs.

## üí° Contributing

Interested in contributing? Contributions to LlamaIndex core as well as contributing
integrations that build on the core are both accepted and highly encouraged! See our [Contribution Guide](CONTRIBUTING.md) for more details.

New integrations should meaningfully integrate with existing LlamaIndex framework components. At the discretion of LlamaIndex maintainers, some integrations may be declined.

## üìÑ Documentation

Full documentation can be found [here](https://docs.llamaindex.ai/en/latest/)

Please check it out for the most up-to-date tutorials, how-to guides, references, and other resources!

## üíª Example Usage

```sh
# custom selection of integrations to work with core
pip install llama-index-core
pip install llama-index-llms-openai
pip install llama-index-llms-replicate
pip install llama-index-embeddings-huggingface
```

Examples are in the `docs/examples` folder. Indices are in the `indices` folder (see list of indices below).

To build a simple vector store index using OpenAI:

```python
import os

os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("YOUR_DATA_DIRECTORY").load_data()
index = VectorStoreIndex.from_documents(documents)
```

To build a simple vector store index using non-OpenAI LLMs, e.g. Llama 2 hosted on [Replicate](https://replicate.com/), where you can easily create a free trial API token:

```python
import os

os.environ["REPLICATE_API_TOKEN"] = "YOUR_REPLICATE_API_TOKEN"

from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.replicate import Replicate
from transformers import AutoTokenizer

# set the LLM
llama2_7b_chat = "meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e"
Settings.llm = Replicate(
    model=llama2_7b_chat,
    temperature=0.01,
    additional_kwargs={"top_p": 1, "max_new_tokens": 300},
)

# set tokenizer to match LLM
Settings.tokenizer = AutoTokenizer.from_pretrained(
    "NousResearch/Llama-2-7b-chat-hf"
)

# set the embed model
Settings.embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5"
)

documents = SimpleDirectoryReader("YOUR_DATA_DIRECTORY").load_data()
index = VectorStoreIndex.from_documents(
    documents,
)
```

To query:

```python
query_engine = index.as_query_engine()
query_engine.query("YOUR_QUESTION")
```

By default, data is stored in-memory.
To persist to disk (under `./storage`):

```python
index.storage_context.persist()
```

To reload from disk:

```python
from llama_index.core import StorageContext, load_index_from_storage

# rebuild storage context
storage_context = StorageContext.from_defaults(persist_dir="./storage")
# load index
index = load_index_from_storage(storage_context)
```

## üîß Dependencies

We use poetry as the package manager for all Python packages. As a result, the
dependencies of each Python package can be found by referencing the `pyproject.toml`
file in each of the package's folders.

```bash
cd <desired-package-folder>
pip install poetry
poetry install --with dev
```

## üìñ Citation

Reference to cite if you use LlamaIndex in a paper:

```
@software{Liu_LlamaIndex_2022,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}
```


LlamaIndex l√† g√¨?
LlamaIndex (tr∆∞·ªõc ƒë√¢y g·ªçi l√† GPT Index) l√† m·ªôt framework (khung c√¥ng c·ª•) gi√∫p b·∫°n d·ªÖ d√†ng x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng s·ª≠ d·ª•ng tr√≠ tu·ªá nh√¢n t·∫°o (AI) ki·ªÉu LLM (Large Language Model, v√≠ d·ª• nh∆∞ ChatGPT) v·ªõi d·ªØ li·ªáu ri√™ng c·ªßa b·∫°n.

V·∫•n ƒë·ªÅ m√† LlamaIndex gi·∫£i quy·∫øt
C√°c m√¥ h√¨nh AI l·ªõn nh∆∞ ChatGPT th∆∞·ªùng ch·ªâ bi·∫øt nh·ªØng g√¨ ƒë∆∞·ª£c hu·∫•n luy·ªán (th∆∞·ªùng l√† d·ªØ li·ªáu c√¥ng khai, ƒë·∫øn m·ªôt th·ªùi ƒëi·ªÉm nh·∫•t ƒë·ªãnh). N·∫øu b·∫°n mu·ªën AI ‚Äúhi·ªÉu‚Äù ho·∫∑c tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu n·ªôi b·ªô, t√†i li·ªáu c√° nh√¢n, file PDF, database, hay website ri√™ng,... th√¨ c·∫ßn m·ªôt c√°ch k·∫øt n·ªëi d·ªØ li·ªáu c·ªßa b·∫°n v√†o AI.
ƒê√≥ ch√≠nh l√† l√Ω do ra ƒë·ªùi c·ªßa LlamaIndex.

T√≠nh nƒÉng n·ªïi b·∫≠t
LlamaIndex cung c·∫•p cho b·∫°n:

K·∫øt n·ªëi d·ªØ li·ªáu (data connectors): K·∫øt n·ªëi v·ªõi nhi·ªÅu ngu·ªìn d·ªØ li·ªáu kh√°c nhau nh∆∞ file PDF, doc, d·ªØ li·ªáu SQL, API, Google Drive,...

T·ªï ch·ª©c d·ªØ li·ªáu th√¥ng minh: Chuy·ªÉn d·ªØ li·ªáu th√†nh d·∫°ng m√† AI d·ªÖ s·ª≠ d·ª•ng (ch·∫≥ng h·∫°n l∆∞u th√†nh ch·ªâ m·ª•c/vector ƒë·ªÉ t√¨m ki·∫øm nhanh).

T√¨m ki·∫øm, truy xu·∫•t th√¥ng minh: H·ªèi g√¨ c≈©ng ƒë∆∞·ª£c, AI s·∫Ω t·ª± ƒë·ªông l·∫•y th√¥ng tin li√™n quan nh·∫•t t·ª´ d·ªØ li·ªáu c·ªßa b·∫°n ƒë·ªÉ tr·∫£ l·ªùi.

D·ªÖ d√†ng t√≠ch h·ª£p v√†o c√°c ·ª©ng d·ª•ng: B·∫°n c√≥ th·ªÉ d√πng LlamaIndex c√πng v·ªõi LangChain, Flask, Docker, ChatGPT, ho·∫∑c t·ª± x√¢y d·ª±ng ·ª©ng d·ª•ng ri√™ng.

Ai n√™n d√πng?
Ng∆∞·ªùi m·ªõi (beginner):
B·∫°n ch·ªâ c·∫ßn v√†i d√≤ng code l√† c√≥ th·ªÉ ‚Äún·∫°p‚Äù d·ªØ li·ªáu v√† b·∫Øt ƒë·∫ßu h·ªèi AI r·ªìi.

Ng∆∞·ªùi c√≥ kinh nghi·ªám (advanced):
LlamaIndex cho ph√©p b·∫°n tu·ª≥ ch·ªânh s√¢u t·ª´ng ph·∫ßn, m·ªü r·ªông th√™m c√°c module ri√™ng.

C√°ch ho·∫°t ƒë·ªông t·ªïng qu√°t
N·∫°p d·ªØ li·ªáu: ƒê∆∞a d·ªØ li·ªáu c·ªßa b·∫°n (file, database, v.v.) v√†o LlamaIndex qua c√°c data connectors.

T·∫°o ch·ªâ m·ª•c (index): LlamaIndex chuy·ªÉn d·ªØ li·ªáu sang d·∫°ng m√† AI d·ªÖ s·ª≠ d·ª•ng (v√≠ d·ª•, l∆∞u th√†nh c√°c vector ƒë·ªÉ sau n√†y t√¨m ki·∫øm nhanh).

Truy v·∫•n (query): Khi b·∫°n nh·∫≠p c√¢u h·ªèi, LlamaIndex s·∫Ω t√¨m ki·∫øm ph·∫ßn d·ªØ li·ªáu ph√π h·ª£p nh·∫•t, g·ª≠i cho AI ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c theo ‚Äúki·∫øn th·ª©c ri√™ng‚Äù c·ªßa b·∫°n.

V√≠ d·ª• c∆° b·∫£n (Python)
1. C√†i ƒë·∫∑t
sh
Sao ch√©p
Ch·ªânh s·ª≠a
pip install llama-index
2. S·ª≠ d·ª•ng v·ªõi OpenAI (ChatGPT)
python
Sao ch√©p
Ch·ªânh s·ª≠a
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"  # Thay b·∫±ng API key c·ªßa b·∫°n

documents = SimpleDirectoryReader("THU_MUC_DU_LIEU_CUA_BAN").load_data()
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()
result = query_engine.query("N·ªôi dung c√¢u h·ªèi c·ªßa b·∫°n")
print(result)
3. L∆∞u l·∫°i ch·ªâ m·ª•c ƒë·ªÉ d√πng v·ªÅ sau
python
Sao ch√©p
Ch·ªânh s·ª≠a
index.storage_context.persist()  # L∆∞u index xu·ªëng ·ªï c·ª©ng
4. T·∫£i l·∫°i ch·ªâ m·ª•c
python
Sao ch√©p
Ch·ªânh s·ª≠a
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
T√≥m t·∫Øt √Ω nghƒ©a
LlamaIndex gi·ªëng nh∆∞ m·ªôt ‚Äúb·ªô n√£o ph·ª• tr·ª£‚Äù gi√∫p AI hi·ªÉu ƒë∆∞·ª£c d·ªØ li·ªáu ri√™ng c·ªßa b·∫°n.

D·ªÖ t√≠ch h·ª£p, nhi·ªÅu plugin, t√†i li·ªáu h∆∞·ªõng d·∫´n r√µ r√†ng.

H·ªó tr·ª£ c·∫£ ng∆∞·ªùi m·ªõi v√† chuy√™n gia x√¢y d·ª±ng s·∫£n ph·∫©m AI d·ª±a tr√™n d·ªØ li·ªáu ri√™ng.

T√†i li·ªáu, c·ªông ƒë·ªìng
T√†i li·ªáu ch√≠nh th·ª©c

LlamaHub (kho plugin t·∫£i d·ªØ li·ªáu)

Discord c·ªông ƒë·ªìng

N·∫øu b·∫°n m·ªõi fork repo n√†y, c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ƒë·ªçc c√°c v√≠ d·ª• trong m·ª•c docs/examples, tham kh·∫£o t√†i li·ªáu h∆∞·ªõng d·∫´n chi ti·∫øt, v√† th·ª≠ ‚Äún·∫°p‚Äù d·ªØ li·ªáu c·ªßa m√¨nh v√†o ƒë·ªÉ h·ªèi AI nh√©!


1. T·ªïng quan: LlamaIndex so v·ªõi c√°c h·ªá th·ªëng RAG kh√°c
LlamaIndex th·ª±c ch·∫•t l√† m·ªôt b·ªô framework chuy√™n s√¢u d√†nh ri√™ng cho vi·ªác x√¢y d·ª±ng ·ª©ng d·ª•ng RAG, trong khi nhi·ªÅu h·ªá th·ªëng RAG kh√°c ch·ªâ cung c·∫•p m·ªôt s·ªë th√†nh ph·∫ßn c∆° b·∫£n, ho·∫∑c l√† c√°c gi·∫£i ph√°p ‚Äúall-in-one‚Äù √≠t t√πy bi·∫øn h∆°n.

C√°c h·ªá th·ªëng RAG ph·ªï bi·∫øn b·∫°n c√≥ th·ªÉ ƒë√£ t√¨m hi·ªÉu nh∆∞:

LangChain

Haystack (deepset.ai)

Chroma / Weaviate / Milvus / Qdrant (ch·ªß y·∫øu l√† vector DB, nh∆∞ng ƒë·ªÅu c√≥ th·ªÉ l√†m RAG v·ªõi plugin)

OpenAI Cookbook/ RAG sample / Llama-Index

ƒêi·ªÉm gi·ªëng nhau
ƒê·ªÅu x√¢y d·ª±ng pipeline v·ªõi c√°c b∆∞·ªõc c∆° b·∫£n: n·∫°p d·ªØ li·ªáu ‚Üí t·∫°o vector ‚Üí t√¨m ki·∫øm ‚Üí tr·∫£ v·ªÅ k·∫øt qu·∫£ cho LLM tr·∫£ l·ªùi.

ƒê·ªÅu h·ªó tr·ª£ nhi·ªÅu ngu·ªìn d·ªØ li·ªáu, vector DB, c√°c LLM kh√°c nhau.

2. ƒêi·ªÉm kh√°c bi·ªát n·ªïi b·∫≠t c·ªßa LlamaIndex
A. Kh√°c bi·ªát v·ªÅ m·ª•c ti√™u v√† ki·∫øn tr√∫c
Ti√™u ch√≠	LlamaIndex	LangChain/Haystack	VectorDB (Chroma, Qdrant,...)
M·ª•c ti√™u	Framework t·∫≠p trung RAG, linh ho·∫°t	General orchestration (workflow, agent, RAG, tool-use,...)	L∆∞u tr·ªØ & t√¨m ki·∫øm vector
Kh·∫£ nƒÉng custom pipeline	M·∫°nh, nhi·ªÅu level tu·ª≥ bi·∫øn	R·∫•t m·∫°nh (nh∆∞ng nhi·ªÅu th√†nh ph·∫ßn, d·ªÖ ‚Äúlo√£ng‚Äù)	Ch·ªâ l√†m ph·∫ßn truy xu·∫•t
D·ªÖ d√πng cho ng∆∞·ªùi m·ªõi	Cao, nhi·ªÅu API high-level, √≠t code	Nhi·ªÅu b∆∞·ªõc, d·ªÖ ‚Äúoverwhelm‚Äù	Kh√¥ng ƒë·ªß cho ng∆∞·ªùi m·ªõi t·ª± l√†m RAG

B. LlamaIndex n·ªïi b·∫≠t ·ªü ch·ªó n√†o?
Modular & Plug & Play:

LlamaIndex chia r√µ ‚Äúcore‚Äù v√† ‚Äúplugin/integration‚Äù.

B·∫°n ch·ªâ c·∫ßn c√†i nh·ªØng module c·∫ßn thi·∫øt (v√≠ d·ª•: LLM, embedding, retriever, loader cho t·ª´ng ngu·ªìn d·ªØ li·ªáu), ti·∫øt ki·ªám dung l∆∞·ª£ng v√† ph√π h·ª£p nhu c·∫ßu.

C·ªông ƒë·ªìng m·∫°nh & kho plugin l·ªõn nh·∫•t:

LlamaHub c√≥ h∆°n 300 plugin ƒë·ªÉ n·∫°p d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªôc-l·∫° nh·∫•t (PDF, notion, SQL, c√°c tool SaaS,...).

Lu√¥n c·∫≠p nh·∫≠t plugin m·ªõi, d·ªÖ g√≥p code.

API nhi·ªÅu c·∫•p ƒë·ªô:

API ‚Äú5 d√≤ng code‚Äù: D√†nh cho ng∆∞·ªùi m·ªõi, ch·ªâ c·∫ßn n·∫°p data ‚Üí h·ªèi ‚Üí xong.

API low-level: D·ªÖ custom t·ª´ng th√†nh ph·∫ßn (retriever, reranker, post-processor,...), ph√π h·ª£p nghi√™n c·ª©u, production.

T√≠ch h·ª£p s·∫µn nhi·ªÅu t√≠nh nƒÉng n√¢ng cao:

Retrieval n√¢ng cao: Hybrid search, Semantic rerank, Query Transform, Multi-modal (text + image), Conversational memory, ...

Index ƒëa d·∫°ng: Kh√¥ng ch·ªâ vector m√† c√≤n List, Tree, Graph index.

Streaming, ‚Äúchunk‚Äù ƒë·ªông, context window,...: D·ªÖ tuning, √≠t ph·∫£i ‚Äúhack‚Äù code.

Kh·∫£ nƒÉng t√≠ch h·ª£p v·ªõi c√°c h·ªá th·ªëng kh√°c:

LlamaIndex t√≠ch h·ª£p m∆∞·ª£t v·ªõi LangChain, ChatGPT plugin, Flask, Docker, Streamlit, FastAPI,...

ƒê·∫∑c bi·ªát, c√≥ th·ªÉ xu·∫•t index ra format cho c√°c vector DB ph·ªï bi·∫øn.

Documentation, Example, C·ªông ƒë·ªìng:

T√†i li·ªáu c·ª±c d·ªÖ hi·ªÉu, nhi·ªÅu example ƒëa d·∫°ng th·ª±c chi·∫øn.

C·ªông ƒë·ªìng active, nhi·ªÅu tutorial, event, workshop.

So v·ªõi LangChain
LangChain m·∫°nh v·ªÅ workflow orchestration (l√†m agent, tool-use, chaining nhi·ªÅu pipeline ph·ª©c t·∫°p), nh∆∞ng code RAG thu·∫ßn trong LangChain th∆∞·ªùng ph·∫£i vi·∫øt nhi·ªÅu h∆°n, c·∫•u h√¨nh ph·ª©c t·∫°p.

LlamaIndex t·ªëi ∆∞u h√≥a pipeline RAG, t·∫≠p trung s√¢u cho b√†i to√°n k·∫øt n·ªëi d·ªØ li·ªáu ri√™ng v·ªõi AI.

So v·ªõi Haystack
Haystack thi√™n v·ªÅ enterprise (Python/Java), t√†i li·ªáu t·ªët nh∆∞ng thi√™n v·ªÅ m√¥ h√¨nh chu·∫©n/truy·ªÅn th·ªëng. C·ªông ƒë·ªìng Vi·ªát √≠t h∆°n, plugin √≠t h∆°n LlamaIndex.

So v·ªõi Vector DB thu·∫ßn
Vector DB ch·ªâ l√† ph·∫ßn ‚Äúkho‚Äù l∆∞u d·ªØ li·ªáu ƒë√£ embed, b·∫°n v·∫´n ph·∫£i t·ª± x√¢y c√°c t·∫ßng c√≤n l·∫°i (data ingestion, query, rerank, context,...). LlamaIndex lo lu√¥n nh·ªØng th·ª© n√†y.

3. Khi n√†o n√™n ch·ªçn LlamaIndex?
Mu·ªën x√¢y ·ª©ng d·ª•ng h·ªèi ƒë√°p tr√™n d·ªØ li·ªáu ri√™ng nhanh, code √≠t, d·ªÖ m·ªü r·ªông.

Mu·ªën t·ªëi ∆∞u t·ª´ng b∆∞·ªõc pipeline RAG (custom retriever, chunking, index, rerank,...).

Mu·ªën t√≠ch h·ª£p AI v√†o ·ª©ng d·ª•ng c·ªßa b·∫°n m√† kh√¥ng b·ªã ‚Äúc·ª©ng nh·∫Øc‚Äù nh∆∞ c√°c d·ªãch v·ª• all-in-one (VD: Zapier, Google Vertex AI Search,...).

Th√≠ch c·ªông ƒë·ªìng m·ªü, d·ªÖ g√≥p √Ω, ƒë√≥ng g√≥p plugin m·ªõi.

4. T√≥m t·∫Øt b·∫£ng so s√°nh
Ti√™u ch√≠	LlamaIndex	LangChain	Haystack	VectorDB
Chuy√™n s√¢u RAG	‚úÖ	‚ùå (ƒëa nƒÉng)	‚úÖ	‚ùå
D·ªÖ d√πng cho ng∆∞·ªùi m·ªõi	‚úÖ	‚ùå	‚úÖ	‚ùå
API high-level	‚úÖ	‚ùå	‚úÖ	‚ùå
Kh·∫£ nƒÉng m·ªü r·ªông	‚úÖ	‚úÖ	‚ö†Ô∏è	‚ö†Ô∏è
Plugin Data Loader	‚úÖ (300+)	~100	<50	‚ùå
Doc, c·ªông ƒë·ªìng	‚úÖ	‚úÖ	‚ö†Ô∏è	‚ö†Ô∏è

5. K·∫øt lu·∫≠n: LlamaIndex ph√π h·ª£p v·ªõi ai?
R·∫•t h·ª£p cho c·∫£ d√¢n k·ªπ thu·∫≠t, PM, startup l·∫´n doanh nghi·ªáp mu·ªën POC nhanh ·ª©ng d·ª•ng RAG v·ªõi d·ªØ li·ªáu n·ªôi b·ªô, sau n√†y v·∫´n c√≥ th·ªÉ scale/tu·ª≥ bi·∫øn s√¢u.

N·∫øu b·∫°n ch·ªâ l√†m RAG cho m·ªôt project nh·ªè, mu·ªën code ng·∫Øn, c·∫•u h√¨nh d·ªÖ, th√¨ LlamaIndex l√† l·ª±a ch·ªçn r·∫•t m·∫°nh.

N·∫øu c·∫ßn orchestration nhi·ªÅu workflow (vd: Agent, Tool-Use, Multi-Chain...) th√¨ n√™n c√¢n nh·∫Øc LangChain ho·∫∑c k·∫øt h·ª£p c·∫£ hai.
